{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":93120,"databundleVersionId":11088421,"sourceType":"competition"},{"sourceId":10785666,"sourceType":"datasetVersion","datasetId":6692897}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch tqdm scikit-learn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:53:05.426506Z","iopub.execute_input":"2025-02-18T17:53:05.426822Z","iopub.status.idle":"2025-02-18T17:53:10.460961Z","shell.execute_reply.started":"2025-02-18T17:53:05.426799Z","shell.execute_reply":"2025-02-18T17:53:10.459880Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import ElectraTokenizer, ElectraForSequenceClassification, AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\n\n# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# KoELECTRA í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ\nMODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\ntokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)\nmodel = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4).to(device)  # 4ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:53:10.462391Z","iopub.execute_input":"2025-02-18T17:53:10.462653Z","iopub.status.idle":"2025-02-18T17:53:14.541049Z","shell.execute_reply.started":"2025-02-18T17:53:10.462627Z","shell.execute_reply":"2025-02-18T17:53:14.540016Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/61.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c0bd2a5f62c443e8da8039a55dfa3cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/263k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfee50f81b7c47a2aafaf2517608ac8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/467 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c559c8c4fdde45b199cfa4b7da1711e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/452M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a72347a456142698cf8c0aff58d22cc"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import ElectraTokenizer\n\n# KoELECTRA ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\nMODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\ntokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)\n\n# íŒŒì¼ ê²½ë¡œ\ntrain_file_path = \"/kaggle/input/aiffel-dl-thon-dktc-online-12/train.csv\"\n\n# ë°ì´í„° ë¡œë“œ\ntrain_df = pd.read_csv(train_file_path)\nmore_train_file_path= \"/kaggle/input/unmlve/train_normal_friend_couple_family.csv\"\nmore_train_df= pd.read_csv(more_train_file_path)\ntrain_df= pd.concat([train_df, more_train_df], ignore_index=True)\n\n# ë¼ë²¨ ì¸ì½”ë”©\nlabel_encoder = LabelEncoder()\ntrain_df[\"label\"] = label_encoder.fit_transform(train_df[\"class\"])\n\n# KoELECTRA ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\nclass KoELECTRADataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            padding=\"max_length\",  # ğŸ”¥ ë³€ê²½: ê³ ì •ëœ íŒ¨ë”© ê¸¸ì´\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\n# ë°ì´í„°ì…‹ ìƒì„± (max_length=256 ì ìš©)\ntrain_dataset = KoELECTRADataset(\n    texts=train_df[\"conversation\"].tolist(),\n    labels=train_df[\"label\"].tolist(),\n    tokenizer=tokenizer,\n    max_length=512,  # ì ìš©ë¨\n)\n\n# DataLoader ìƒì„±\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# ë°ì´í„°ì…‹ í¬ê¸° í™•ì¸\nprint(f\"ì´ ë°ì´í„° ê°œìˆ˜: {len(train_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:53:14.543224Z","iopub.execute_input":"2025-02-18T17:53:14.543582Z","iopub.status.idle":"2025-02-18T17:53:14.772200Z","shell.execute_reply.started":"2025-02-18T17:53:14.543540Z","shell.execute_reply":"2025-02-18T17:53:14.771305Z"}},"outputs":[{"name":"stdout","text":"ì´ ë°ì´í„° ê°œìˆ˜: 4550\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# pred_label ì»¬ëŸ¼ì˜ ê°’ ê°œìˆ˜ ì„¸ê¸°\nlabel_counts = train_df['class'].value_counts()\n\n# ê²°ê³¼ ì¶œë ¥\nprint(label_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:53:14.773162Z","iopub.execute_input":"2025-02-18T17:53:14.773557Z","iopub.status.idle":"2025-02-18T17:53:14.786240Z","shell.execute_reply.started":"2025-02-18T17:53:14.773527Z","shell.execute_reply":"2025-02-18T17:53:14.785133Z"}},"outputs":[{"name":"stdout","text":"class\nê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”      1094\nê°ˆì·¨ ëŒ€í™”           981\nì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”     979\ní˜‘ë°• ëŒ€í™”           896\nì¼ë°˜ ëŒ€í™”           600\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import ElectraForSequenceClassification, AdamW\nfrom tqdm import tqdm\n\n# KoELECTRA ëª¨ë¸ ë¡œë“œ\nmodel = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=5).to(device)\n\n# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì •ì˜\ncriterion = nn.CrossEntropyLoss()\noptimizer = AdamW(model.parameters(), lr=3e-5)\n\n# í•™ìŠµ ë£¨í”„\nnum_epochs = 15\nbest_loss = float(\"inf\")\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        optimizer.zero_grad()\n\n        # ë°°ì¹˜ ë°ì´í„° GPUë¡œ ì´ë™\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # ëª¨ë¸ ì¶œë ¥\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        # ì—­ì „íŒŒ & ìµœì í™”\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        loop.set_description(f\"Epoch {epoch+1}\")\n        loop.set_postfix(loss=loss.item())\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n\n    # ëª¨ë¸ ì €ì¥ (ìµœì  ëª¨ë¸)\n    if avg_loss < best_loss:\n        best_loss = avg_loss\n        torch.save(model.state_dict(), \"/kaggle/working/best_model.pt\")\n        print(\"âœ… Model Saved!\")\n\nprint(\"ğŸ‰ Training Finished!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:53:14.787232Z","iopub.execute_input":"2025-02-18T17:53:14.787495Z","iopub.status.idle":"2025-02-18T18:47:43.043394Z","shell.execute_reply.started":"2025-02-18T17:53:14.787462Z","shell.execute_reply":"2025-02-18T18:47:43.041954Z"}},"outputs":[{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:04<00:00,  1.16it/s, loss=0.12]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.7786\nâœ… Model Saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:05<00:00,  1.16it/s, loss=0.042] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.2440\nâœ… Model Saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:04<00:00,  1.16it/s, loss=0.0225]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.1484\nâœ… Model Saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:05<00:00,  1.16it/s, loss=0.00581]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0984\nâœ… Model Saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:05<00:00,  1.16it/s, loss=0.00903]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0705\nâœ… Model Saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:05<00:00,  1.16it/s, loss=0.00216]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0351\nâœ… Model Saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:05<00:00,  1.16it/s, loss=0.97]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0715\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:05<00:00,  1.16it/s, loss=0.0177] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0456\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:05<00:00,  1.16it/s, loss=0.00471]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0304\nâœ… Model Saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:05<00:00,  1.16it/s, loss=0.0021] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0236\nâœ… Model Saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:05<00:00,  1.16it/s, loss=0.0029]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 Loss: 0.0237\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:05<00:00,  1.16it/s, loss=0.00394]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 Loss: 0.0406\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [04:05<00:00,  1.16it/s, loss=0.00295]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 Loss: 0.0260\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14:  28%|â–ˆâ–ˆâ–Š       | 81/285 [01:10<02:57,  1.15it/s, loss=0.0019]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-488f49a23e41>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# ì—­ì „íŒŒ & ìµœì í™”\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import ElectraForSequenceClassification\n\n# íŒŒì¼ ê²½ë¡œ\ntest_file_path = \"/kaggle/input/aiffel-dl-thon-dktc-online-12/test.csv\"\nsubmission_file_path = \"/kaggle/working/submission.csv\"\nmodel_path = \"/kaggle/working/best_model.pt\"\n\n# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\ntest_df = pd.read_csv(test_file_path)\n\n# ëª¨ë¸ ë¡œë“œ\nmodel = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=5).to(device)\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n\n# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\nclass TestKoELECTRADataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=512,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n        }\n\n# ğŸ”¥ test_df[\"conversation\"] â†’ test_df[\"text\"]ë¡œ ìˆ˜ì •\ntest_dataset = TestKoELECTRADataset(\n    texts=test_df[\"text\"].tolist(),  # âœ… ì—¬ê¸° ìˆ˜ì •ë¨!\n    tokenizer=tokenizer,\n    max_length=512,\n)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# ì˜ˆì¸¡ ìˆ˜í–‰\npredictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        predictions.extend(preds.cpu().numpy())\n\n# ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ìˆ«ìë¥¼ ì›ë˜ ë¼ë²¨(`class`)ë¡œ ë³€í™˜\ntest_df[\"class\"] = label_encoder.inverse_transform(predictions)\n\n# ê²°ê³¼ ì €ì¥\ntest_df[[\"idx\", \"class\"]].to_csv(submission_file_path, index=False)\nprint(f\"âœ… ì˜ˆì¸¡ ì™„ë£Œ! ê²°ê³¼ ì €ì¥: {submission_file_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:02:54.084361Z","iopub.execute_input":"2025-02-18T19:02:54.084664Z","iopub.status.idle":"2025-02-18T19:03:04.514609Z","shell.execute_reply.started":"2025-02-18T19:02:54.084641Z","shell.execute_reply":"2025-02-18T19:03:04.513307Z"}},"outputs":[{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-39-1fc9ec158376>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"},{"name":"stdout","text":"âœ… ì˜ˆì¸¡ ì™„ë£Œ! ê²°ê³¼ ì €ì¥: /kaggle/working/submission.csv\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport torch.nn.functional as F\n\n\n\n\n# ê¸°ì¡´ ì„œë¸Œë¯¸ì…˜ íŒŒì¼ ë¡œë“œ\nsubmission_path = \"/kaggle/working/submission.csv\"\nsubmission_df = pd.read_csv(submission_path)\n\n# ëª¨ë¸ ë¡œë“œ\nmodel_path = \"/kaggle/working/best_model.pt\"\nmodel = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=5).to(device)\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\n\n# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\ntest_file_path = \"/kaggle/input/aiffel-dl-thon-dktc-online-12/test.csv\"\ntest_df = pd.read_csv(test_file_path)\n\n# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\nclass TestKoELECTRADataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=512,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n        }\n\n# ë°ì´í„° ë¡œë”©\ntest_dataset = TestKoELECTRADataset(\n    texts=test_df[\"text\"].tolist(),\n    tokenizer=tokenizer,\n    max_length=512,\n)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# ì˜ˆì¸¡ ìˆ˜í–‰\npredictions = []\nprobabilities = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        probs = F.softmax(outputs.logits, dim=1)  # í™•ë¥  ë³€í™˜\n\n        max_probs, preds = torch.max(probs, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        probabilities.extend(max_probs.cpu().numpy())\n\n# í´ë˜ìŠ¤ ë§¤í•‘\nclass_mapping = {\n    0: \"ê°ˆì·¨ ëŒ€í™”\",\n    1: \"ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”\",\n    2: \"ì¼ë°˜ ëŒ€í™”\",\n    3: \"ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”\",\n    4: \"í˜‘ë°• ëŒ€í™”\",\n}\n\n# Softmax í™•ë¥ ì´ 0.3 ì´í•˜ì¸ ê²½ìš° ì¼ë°˜ ëŒ€í™”ë¡œ ìë™ ë³´ì •\nfor i in range(len(probabilities)):\n    if probabilities[i] < 0.9:\n        predictions[i] = 2  # ì¼ë°˜ ëŒ€í™”\n\n# ìµœì¢… ê²°ê³¼ ì €ì¥\ntest_df[\"class\"] = [class_mapping[p] for p in predictions]\ntest_df[[\"idx\", \"class\"]].to_csv(\"/kaggle/working/submission_adjusted.csv\", index=False)\n\nprint(\"âœ… ì¼ë°˜ ëŒ€í™” ê°•í™” í›„ ì„œë¸Œë¯¸ì…˜ ì €ì¥ ì™„ë£Œ! ğŸš€\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:07:22.858221Z","iopub.execute_input":"2025-02-18T19:07:22.858579Z","iopub.status.idle":"2025-02-18T19:07:33.393195Z","shell.execute_reply.started":"2025-02-18T19:07:22.858551Z","shell.execute_reply":"2025-02-18T19:07:33.391910Z"}},"outputs":[{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-53-03f7461e7508>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"},{"name":"stdout","text":"âœ… ì¼ë°˜ ëŒ€í™” ê°•í™” í›„ ì„œë¸Œë¯¸ì…˜ ì €ì¥ ì™„ë£Œ! ğŸš€\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"# softmax ê¸°ë°˜ ì²˜ë¦¬ ì „ submission ê²°ê³¼: 0.68678","metadata":{}},{"cell_type":"code","source":"#softmax ê¸°ë°˜ í•„í„°ë§ ë¯¸ì ìš©\n# pred_label ì»¬ëŸ¼ì˜ ê°’ ê°œìˆ˜ ì„¸ê¸°\nlabel_counts = test_df['class'].value_counts()\n\n# ê²°ê³¼ ì¶œë ¥\nprint(label_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:03:38.141148Z","iopub.execute_input":"2025-02-18T19:03:38.141501Z","iopub.status.idle":"2025-02-18T19:03:38.148384Z","shell.execute_reply.started":"2025-02-18T19:03:38.141469Z","shell.execute_reply":"2025-02-18T19:03:38.147246Z"}},"outputs":[{"name":"stdout","text":"class\nê°ˆì·¨ ëŒ€í™”          156\nê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”      134\ní˜‘ë°• ëŒ€í™”          105\nì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”     99\nì¼ë°˜ ëŒ€í™”            6\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"# softmax ì²˜ë¦¬ í›„ submission ê²°ê³¼: 0.69439  \n# í›„ì²˜ë¦¬ ê¸°ë²• ë³„ ì˜í–¥ ì—†ëŠ”ë“¯","metadata":{}},{"cell_type":"code","source":"#softmax ê¸°ë°˜ í•„í„°ë§ ì ìš©\n# pred_label ì»¬ëŸ¼ì˜ ê°’ ê°œìˆ˜ ì„¸ê¸°\nlabel_counts = test_df['class'].value_counts()\n\n# ê²°ê³¼ ì¶œë ¥\nprint(label_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:07:33.395187Z","iopub.execute_input":"2025-02-18T19:07:33.395586Z","iopub.status.idle":"2025-02-18T19:07:33.402791Z","shell.execute_reply.started":"2025-02-18T19:07:33.395551Z","shell.execute_reply":"2025-02-18T19:07:33.401936Z"}},"outputs":[{"name":"stdout","text":"class\nê°ˆì·¨ ëŒ€í™”          146\nê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”      119\ní˜‘ë°• ëŒ€í™”           97\nì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”     92\nì¼ë°˜ ëŒ€í™”           46\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"\nsubmission_df = test_df\n\n# í´ë˜ìŠ¤ ë§¤í•‘ ì •ì˜\nclass_mapping = {\n    \"í˜‘ë°• ëŒ€í™”\": \"00\",\n    \"ê°ˆì·¨ ëŒ€í™”\": \"01\",\n    \"ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”\": \"02\",\n    \"ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”\": \"03\",\n    \"ì¼ë°˜ ëŒ€í™”\": \"04\",\n}\n\n# í´ë˜ìŠ¤ëª… â†’ ìˆ«ìë¡œ ë³€í™˜\nsubmission_df[\"class_no\"] = submission_df[\"class\"].map(class_mapping)\nsubmission_df.drop(columns=[\"class\"], inplace=True)\n# ì»¬ëŸ¼ëª… ë³€í™˜\nsubmission_df = submission_df.rename(columns={\"class_no\": \"class\"})\n\n# ìµœì¢… í˜•ì‹ ë§ì¶”ê¸°\nsubmission_df = submission_df[[\"idx\", \"class\"]]\n\n# ìƒˆë¡œìš´ ì„œë¸Œë¯¸ì…˜ íŒŒì¼ ì €ì¥\nsubmission_final_path = \"submission_final.csv\"\nsubmission_df.to_csv(submission_final_path, index=False)\n\nprint(f\"âœ… ì„œë¸Œë¯¸ì…˜ íŒŒì¼ ë³€í™˜ ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: {submission_final_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:07:57.385495Z","iopub.execute_input":"2025-02-18T19:07:57.385891Z","iopub.status.idle":"2025-02-18T19:07:57.398486Z","shell.execute_reply.started":"2025-02-18T19:07:57.385832Z","shell.execute_reply":"2025-02-18T19:07:57.397408Z"}},"outputs":[{"name":"stdout","text":"âœ… ì„œë¸Œë¯¸ì…˜ íŒŒì¼ ë³€í™˜ ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: submission_final.csv\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\nsubmission_file_path_5 = \"/kaggle/working/submission_5.csv\"\n\n# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\nclass TestKoELECTRADataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=256,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n        }\n\n# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±\ntest_dataset = TestKoELECTRADataset(\n    texts=test_df[\"text\"].tolist(),\n    tokenizer=tokenizer,\n    max_length=256,\n)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# ì˜ˆì¸¡ ìˆ˜í–‰\npredictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        probs = F.softmax(logits, dim=1)  # í™•ë¥  ë³€í™˜\n\n        max_probs, preds = torch.max(probs, dim=1)\n        predictions.extend(zip(preds.cpu().numpy(), max_probs.cpu().numpy()))\n\n# ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ìˆ«ìë¥¼ ì›ë˜ ë¼ë²¨(`class`)ë¡œ ë³€í™˜\npred_labels = [label_encoder.inverse_transform([p[0]])[0] for p in predictions]\npred_probs = [p[1] for p in predictions]\n\n# ì¼ë°˜ ëŒ€í™” í•„í„°ë§ ê¸°ì¤€: í™•ë¥ ì´ ì¼ì • ì„ê³„ê°’ ì´í•˜ì¼ ê²½ìš°\nthreshold = 0.8  # í™•ì‹ ì´ ë‚®ì€ ìƒ˜í”Œì„ ì¼ë°˜ ëŒ€í™”ë¡œ ê°„ì£¼\nfor i in range(len(pred_probs)):\n    if pred_probs[i] < threshold:\n        pred_labels[i] = \"ì¼ë°˜ ëŒ€í™”\"\n\n# ê²°ê³¼ ì €ì¥\ntest_df[\"class\"] = pred_labels\ntest_df[[\"idx\", \"class\"]].to_csv(submission_file_path_5, index=False)\nprint(f\"âœ… ì˜ˆì¸¡ ì™„ë£Œ! ì¼ë°˜ ëŒ€í™” í•„í„°ë§ ì ìš©ë¨. ê²°ê³¼ ì €ì¥: {submission_file_path_5}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:42:18.511838Z","iopub.execute_input":"2025-02-18T07:42:18.512127Z","iopub.status.idle":"2025-02-18T07:42:22.820890Z","shell.execute_reply.started":"2025-02-18T07:42:18.512106Z","shell.execute_reply":"2025-02-18T07:42:22.819971Z"}},"outputs":[{"name":"stdout","text":"âœ… ì˜ˆì¸¡ ì™„ë£Œ! ì¼ë°˜ ëŒ€í™” í•„í„°ë§ ì ìš©ë¨. ê²°ê³¼ ì €ì¥: /kaggle/working/submission_5.csv\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn.functional as F\n\n# í•™ìŠµ ë°ì´í„°ì—ì„œ ê° í´ë˜ìŠ¤ì˜ í‰ê·  ë²¡í„°(Î¼)ì™€ ê³µë¶„ì‚° í–‰ë ¬(Î£) êµ¬í•˜ê¸°\nclass_means = {}  # ê° í´ë˜ìŠ¤ë³„ í‰ê·  ë²¡í„°\nclass_cov_inv = {}  # ê° í´ë˜ìŠ¤ë³„ ê³µë¶„ì‚° í–‰ë ¬ì˜ ì—­í–‰ë ¬\n\nfeatures = {label: [] for label in label_encoder.classes_}  # í´ë˜ìŠ¤ë³„ íŠ¹ì§• ë²¡í„° ì €ì¥\n\nwith torch.no_grad():\n    for batch in train_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].cpu().numpy()\n\n        # ëª¨ë¸ì˜ ë§ˆì§€ë§‰ íˆë“  ìŠ¤í…Œì´íŠ¸ ê°€ì ¸ì˜¤ê¸°\n        outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n        hidden_states = outputs.hidden_states[-1]  # ë§ˆì§€ë§‰ íˆë“  ë ˆì´ì–´\n        embeddings = hidden_states.mean(dim=1).cpu().numpy()  # í‰ê·  í’€ë§\n\n        # ê° í´ë˜ìŠ¤ë³„ íŠ¹ì§• ë²¡í„° ì €ì¥\n        for i, label in enumerate(labels):\n            features[label_encoder.inverse_transform([label])[0]].append(embeddings[i])\n\n# ê° í´ë˜ìŠ¤ë³„ í‰ê·  ë²¡í„° & ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚°\nfor label in features:\n    class_means[label] = np.mean(features[label], axis=0)\n    cov_matrix = np.cov(np.array(features[label]).T)\n    class_cov_inv[label] = np.linalg.pinv(cov_matrix + np.eye(cov_matrix.shape[0]) * 1e-6)  # ì•ˆì •ì  ì—­í–‰ë ¬ ê³„ì‚°\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T11:01:28.297336Z","iopub.execute_input":"2025-02-18T11:01:28.297616Z","iopub.status.idle":"2025-02-18T11:02:03.186824Z","shell.execute_reply.started":"2025-02-18T11:01:28.297593Z","shell.execute_reply":"2025-02-18T11:02:03.185792Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Mahalanobis ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜\ndef mahalanobis_distance(x, mean, cov_inv):\n    delta = x - mean\n    return np.sqrt(np.dot(np.dot(delta, cov_inv), delta.T))\n\n# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ Mahalanobis Distance ê³„ì‚°\ndistances = []\npredictions = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n        hidden_states = outputs.hidden_states[-1]  # ë§ˆì§€ë§‰ íˆë“  ë ˆì´ì–´ ë²¡í„°\n        embeddings = hidden_states.mean(dim=1).cpu().numpy()  # í‰ê·  í’€ë§\n\n        # Mahalanobis ê±°ë¦¬ ê³„ì‚°\n        batch_preds = []\n        for emb in embeddings:\n            min_dist = float(\"inf\")\n            best_class = None\n            for class_label in class_means:\n                dist = mahalanobis_distance(emb, class_means[class_label], class_cov_inv[class_label])\n                if dist < min_dist:\n                    min_dist = dist\n                    best_class = class_label\n            distances.append(min_dist)\n            batch_preds.append(best_class)\n\n        predictions.extend(batch_preds)\n\n# Mahalanobis ì„ê³„ê°’ ì„¤ì • (95% ì´ìƒ ë²—ì–´ë‚œ ë°ì´í„°ëŠ” ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜)\nthreshold = np.percentile(distances, 85)\nfor i in range(len(distances)):\n    if distances[i] > threshold:\n        predictions[i] = \"ì¼ë°˜ ëŒ€í™”\"\n\n# ê²°ê³¼ ì €ì¥\ntest_df[\"class\"] = predictions\ntest_df[[\"idx\", \"class\"]].to_csv(\"/kaggle/working/submission_mahalanobis.csv\", index=False)\nprint(\"âœ… Mahalanobis Distance ê¸°ë°˜ ì˜ˆì¸¡ ì™„ë£Œ! ê²°ê³¼ ì €ì¥ë¨.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:53:51.560771Z","iopub.execute_input":"2025-02-18T08:53:51.561055Z","iopub.status.idle":"2025-02-18T08:53:56.230083Z","shell.execute_reply.started":"2025-02-18T08:53:51.561034Z","shell.execute_reply":"2025-02-18T08:53:56.229153Z"}},"outputs":[{"name":"stdout","text":"âœ… Mahalanobis Distance ê¸°ë°˜ ì˜ˆì¸¡ ì™„ë£Œ! ê²°ê³¼ ì €ì¥ë¨.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"from scipy.stats import weibull_min\n\n# ê° í´ë˜ìŠ¤ë³„ ìµœê³  ì ìˆ˜ ë¶„í¬(Weibull Distribution) ê³„ì‚°\nweibull_params = {}\n\nfor label in features:\n    max_logits = []\n    with torch.no_grad():\n        for batch in train_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].cpu().numpy()\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits.cpu().numpy()\n\n            for i, label_idx in enumerate(labels):\n                if label_encoder.inverse_transform([label_idx])[0] == label:\n                    max_logits.append(np.max(logits[i]))  # ìµœê³  ì ìˆ˜ ì €ì¥\n\n    # Weibull ë¶„í¬ í”¼íŒ…\n    shape, loc, scale = weibull_min.fit(max_logits, floc=0)\n    weibull_params[label] = (shape, scale)\n\n# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ OpenMax ì ìš©\nopenmax_preds = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits.cpu().numpy()\n\n        for i, logit in enumerate(logits):\n            max_score = np.max(logit)\n            best_class = label_encoder.inverse_transform([np.argmax(logit)])[0]\n\n            # Weibull ë¶„í¬ì—ì„œ ë²—ì–´ë‚˜ë©´ ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜\n            shape, scale = weibull_params[best_class]\n            weibull_cdf = weibull_min.cdf(max_score, shape, scale=scale)\n\n            if weibull_cdf < 0.05:  # 5% í™•ë¥  ì´í•˜ì¸ ë°ì´í„°ëŠ” ì¼ë°˜ ëŒ€í™”ë¡œ ì„¤ì •\n                openmax_preds.append(\"ì¼ë°˜ ëŒ€í™”\")\n            else:\n                openmax_preds.append(best_class)\n\n# ê²°ê³¼ ì €ì¥\ntest_df[\"class\"] = openmax_preds\ntest_df[[\"idx\", \"class\"]].to_csv(\"/kaggle/working/submission_openmax.csv\", index=False)\nprint(\"âœ… OpenMax ê¸°ë°˜ ì˜ˆì¸¡ ì™„ë£Œ! ê²°ê³¼ ì €ì¥ë¨.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T11:02:20.578021Z","iopub.execute_input":"2025-02-18T11:02:20.578355Z","iopub.status.idle":"2025-02-18T11:04:41.299613Z","shell.execute_reply.started":"2025-02-18T11:02:20.578333Z","shell.execute_reply":"2025-02-18T11:04:41.298323Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-4bd3ca5c8d64>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# ê²°ê³¼ ì €ì¥\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenmax_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"idx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"class\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/mnt/data/submission_openmax.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ… OpenMax ê¸°ë°˜ ì˜ˆì¸¡ ì™„ë£Œ! ê²°ê³¼ ì €ì¥ë¨.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3965\u001b[0m         )\n\u001b[1;32m   3966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3967\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3968\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         )\n\u001b[0;32m-> 1014\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/mnt/data'"],"ename":"OSError","evalue":"Cannot save file into a non-existent directory: '/mnt/data'","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"\n\n# pred_label ì»¬ëŸ¼ì˜ ê°’ ê°œìˆ˜ ì„¸ê¸°\nlabel_counts = test_df['class'].value_counts()\n\n# ê²°ê³¼ ì¶œë ¥\nprint(label_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T16:23:15.095937Z","iopub.execute_input":"2025-02-18T16:23:15.096380Z","iopub.status.idle":"2025-02-18T16:23:15.106377Z","shell.execute_reply.started":"2025-02-18T16:23:15.096341Z","shell.execute_reply":"2025-02-18T16:23:15.105454Z"}},"outputs":[{"name":"stdout","text":"class\nê°ˆì·¨ ëŒ€í™”          132\ní˜‘ë°• ëŒ€í™”          126\nê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”      125\nì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”    113\nì¼ë°˜ ëŒ€í™”            4\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"test_df[[\"idx\", \"class\"]].to_csv(\"/kaggle/working/submission_openmax.csv\", index=False)\nprint(\"âœ… OpenMax ê¸°ë°˜ ì˜ˆì¸¡ ì™„ë£Œ! ê²°ê³¼ ì €ì¥ë¨.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T11:05:31.480076Z","iopub.execute_input":"2025-02-18T11:05:31.480377Z","iopub.status.idle":"2025-02-18T11:05:31.487580Z","shell.execute_reply.started":"2025-02-18T11:05:31.480355Z","shell.execute_reply":"2025-02-18T11:05:31.486759Z"}},"outputs":[{"name":"stdout","text":"âœ… OpenMax ê¸°ë°˜ ì˜ˆì¸¡ ì™„ë£Œ! ê²°ê³¼ ì €ì¥ë¨.\n","output_type":"stream"}],"execution_count":11}]}